---
title: "STA260_datacleaning"
author: "YL"
date: '2022-05-29'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```

```{r}
library(dplyr)
library(zoo)
library(ggplot2)
library(plotly)
library(hrbrthemes)
library(RColorBrewer)
```

## Data cleaning
```{r, warning=FALSE}
## read all data files
setwd('..')
folder_path = list.dirs(path='./data')[-1]
# category label for each youtube channel
category_label = c()
# outer join all files on start_date
# initiate empty data frames for weekly subscribers and weekly views
df_subscribers = data.frame(start_date=character())
df_views = data.frame(start_date=character())
# iterate over folders of different categories to gather all the files
for (folder in folder_path){
  # category label of the current folder
  label = tail(unlist(strsplit(folder,split='/',fixed=TRUE)),n=1)
  # get all the files inside a folder
  files = list.files(path=folder,pattern="*.csv")
  for (i in 1:length(files)){
    file_name=unlist(strsplit(files[i],split='.',fixed=TRUE))[1]
    new_df=read.csv(paste(folder,files[i],sep='/'))
    new_df_subscribers = new_df[,c(1,3)]
    colnames(new_df_subscribers)[2]=file_name
    df_subscribers = df_subscribers%>%full_join(new_df_subscribers,by=colnames(new_df)[1])
    new_df_views = new_df[,c(1,4)]
    colnames(new_df_views)[2]=file_name
    df_views = df_views%>%full_join(new_df_views,by=colnames(new_df)[1])
    # add label of the current file to the label vector
    category_label=c(category_label,label)
  }}
#----------------------------------------------------------------------------------------------------
## align dates
# sort dataframes on start_date
df_subscribers$start_date=as.Date(df_subscribers$start_date)
df_subscribers=df_subscribers%>%arrange(start_date)
df_views$start_date=as.Date(df_views$start_date)
df_views = df_views%>%arrange(start_date)

# remove rows with date before 2019/06/01
df_subscribers_cleaned = df_subscribers[df_subscribers$start_date>as.Date('2019-05-25'),]
df_views_cleaned = df_views[df_views$start_date>as.Date('2019-05-25'),]

# find columns with more than 10 consecutive NAs
weird_columns = c()
for (i in 2:dim(df_subscribers_cleaned)[2]){
  # calculate the moving row averages with a window of 10 rows for each column. If there are 10 consecutive NAs, then the moving average would be NA too
  rm_10 = rollmean(df_subscribers_cleaned[,i],k=10,na.rm=TRUE)
  rm_10_views = rollmean(df_views_cleaned[,i],k=10,na.rm=TRUE)
  if (is.na(sum(rm_10))|is.na(sum(rm_10_views))){
    weird_columns=c(weird_columns,i)
  }
}
# check which category these channels are from
catg_weird = category_label[weird_columns-1] # 1 comedy, 3 commentary, 1 news, 1 travel
# remove channels with different time frames
df_subscribers_cleaned=df_subscribers_cleaned[,-weird_columns]
df_views_cleaned=df_views_cleaned[,-weird_columns]
category_label=category_label[-(weird_columns-1)]
#----------------------------------------------------------------------------------------------------
## clean the start_date: right now different channels can have data collected on different days within a week. After cleaning, all the start_date will be 7 days apart. If for a channel, no data was recorded for a certain date, the data recorded within 6 days after this date was used to substitute the missing value. 
# set a new set of dates that are all 7 days apart
set_dates = seq(as.Date('2019-05-26'),as.Date('2022-05-15'),by='week')
# initiate empty data frames
df_subscribers_aligned=as.data.frame(matrix(nrow=length(set_dates),ncol=dim(df_subscribers_cleaned)[2]))
df_views_aligned=as.data.frame(matrix(nrow=length(set_dates),ncol=dim(df_views_cleaned)[2]))
colnames(df_subscribers_aligned)=colnames(df_subscribers_cleaned)
colnames(df_views_aligned)=colnames(df_views_cleaned)
df_subscribers_aligned$start_date=set_dates
df_views_aligned$start_date=set_dates
# clean data as described above
old_dates=df_subscribers_cleaned$start_date
for (j in 2:dim(df_subscribers_cleaned)[2]){
  column_subscribers = df_subscribers_cleaned[,j]
  column_views = df_views_cleaned[,j]
  for (i in 1: length(set_dates)){
    new_date = set_dates[i]
    value_subscribers = column_subscribers[which(old_dates==new_date)]
    value_views = column_views[which(old_dates==new_date)]
    if (!is.na(value_subscribers)){
      df_subscribers_aligned[i,j]=value_subscribers
    }
    else{
      df_subscribers_aligned[i,j]=mean(column_subscribers[which(old_dates>=new_date & old_dates<new_date+7)],na.rm=TRUE)
    }
    if (!is.na(value_views)){
      df_views_aligned[i,j]=value_views
    }
    else{
      df_views_aligned[i,j]=mean(column_views[which(old_dates>=new_date & old_dates<new_date+7)],na.rm=TRUE)
    }
}
}
df_subscribers_aligned=na.omit(df_subscribers_aligned)
df_views_aligned=na.omit(df_views_aligned)
#----------------------------------------------------------------------------------------------------
## replace abnormal weekly gained views: abnormal values are defined as: 1). negative views 2).absolute value greater than 10 times of the column median 
for (j in 2:dim(df_views_aligned)[2]){
  col_median=median(df_views_aligned[,j])
  index_adjust = which(df_views_aligned[,j]<0 | abs(df_views_aligned[,j])>10*col_median)
  if (length(index_adjust)!=0){
    for (i in index_adjust){
      df_views_aligned[i,j]=mean(df_views_aligned[c((i-2),(i-1),(i+1),(i+2)),j])
    }
  }
}

## replace abnormal weekly gained subscribers: abnormal values are defined as: absolute value greater than 10 times of mean absolute value
for (j in 2:dim(df_subscribers_aligned)[2]){
  col_mean=mean(abs(df_subscribers_aligned[,j])) 
  index_adjust = which(abs(df_subscribers_aligned[,j])>10*abs(col_mean))
  if (length(index_adjust)!=0){
    for (i in index_adjust){
      df_subscribers_aligned[i,j]=mean(df_subscribers_aligned[c((i-2),(i-1),(i+1),(i+2)),j])
    }
  }
}
```

## Pre-processing
```{r}
## deal with the low resolution of weekly gained subscribers data
fill_in_blanks<-function(series){
  new_series=series
  mask = series
  mask[which(mask!=0)]=1
  last_non_zero=which(mask[-length(mask)]>mask[-1])
  first_non_zero=which(mask[-length(mask)]<mask[-1])+1
  if (length(first_non_zero)!=0){
    for (i in 1: min(length(first_non_zero),length(last_non_zero))){
      value=series[first_non_zero[i]]
      if (first_non_zero[i]<=last_non_zero[i]){
        if(i==1){start=0}
        else{start=last_non_zero[i-1]}
      }
      else{start=last_non_zero[i]}
      n_weeks=first_non_zero[i]-start
      avg = value/n_weeks
      new_series[(start+1):first_non_zero[i]]=avg
    }
  }
  return(as.numeric(new_series))
}

df_subscribers_interpolated = df_subscribers_aligned
for (j in 2:dim(df_subscribers_aligned)[2]){
  df_subscribers_interpolated[,j]=fill_in_blanks(df_subscribers_aligned[,j])
}

## smooth the curves with moving averages 
df_subscribers_smoothed= as.data.frame(apply(df_subscribers_interpolated[,-1],2,function(x) round(rollmean(x,k=13,fill=NA,na.rm=TRUE),digits=0)))
df_subscribers_smoothed$start_date = df_subscribers_aligned$start_date
df_subscribers_smoothed=df_subscribers_smoothed[,c(dim(df_subscribers_smoothed)[2],1:dim(df_subscribers_smoothed)[2]-1)]
df_subscribers_smoothed=na.omit(df_subscribers_smoothed)

df_views_smoothed= as.data.frame(apply(df_views_aligned[,-1],2,function(x) round(rollmean(x,k=13,fill=NA,na.rm=TRUE),digits=0)))
df_views_smoothed$start_date = df_views_aligned$start_date
df_views_smoothed=df_views_smoothed[,c(dim(df_views_smoothed)[2],1:dim(df_views_smoothed)[2]-1)]
df_views_smoothed=na.omit(df_views_smoothed)
#----------------------------------------------------------------------------------------------------
## min_max normalization of the data
norm_minmax <- function(x){(x- min(x)) /(max(x)-min(x))}

df_subscribers_normalized=cbind(df_subscribers_smoothed$start_date,as.data.frame(lapply(df_subscribers_smoothed[,-1],norm_minmax)))
colnames(df_subscribers_normalized)[1]='start_date'

df_views_normalized=cbind(df_views_smoothed$start_date,as.data.frame(lapply(df_views_smoothed[,-1],norm_minmax)))
colnames(df_views_normalized)[1]='start_date'
```

```{r}
# quarterly average by category
# subscribers
df_normalized_long$quarter = 0
for (i in 1:dim(df_normalized_long)[1]){
  df_normalized_long$quarter[i]=as.numeric(df_normalized_long$start_date[i]-df_normalized_long$start_date[1])%/%91+1
}
group_median = df_normalized_long%>%group_by(category,start_date)%>%summarise(quarter = mean(quarter),group_median_sub=median(weekly_subscribers),group_median_view = median(weekly_views))

group_quarterly_median = group_median%>%group_by(quarter,category)%>%summarise(quarterly_median_sub=median(group_median_sub),quarterly_median_view=median(group_median_view))
```


## EDA
### comparison before and after cleaning
```{r, warnings=FALSE}
# covert tables to long version and combine weekly subscribers and views to the same data frame
# aligned data
df_subscribers_aligned_long=pivot_longer(df_subscribers_aligned,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_subscribers')
df_views_aligned_long=pivot_longer(df_views_aligned,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_views')
df_aligned_long = df_subscribers_aligned_long%>%inner_join(df_views_aligned_long,by=c('start_date','name'))
df_aligned_long$category = rep(category_label,dim(df_subscribers_aligned)[1])
# interpolated subscribers data
df_subscribers_interpolated_long = pivot_longer(df_subscribers_interpolated,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_subscribers')
df_subscribers_interpolated_long$category = rep(category_label,dim(df_subscribers_aligned)[1])
# smoothed data
df_subscribers_smoothed_long=pivot_longer(df_subscribers_smoothed,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_subscribers')
df_views_smoothed_long=pivot_longer(df_views_smoothed,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_views')
df_smoothed_long = df_subscribers_smoothed_long%>%inner_join(df_views_smoothed_long,by=c('start_date','name'))
df_smoothed_long$category = rep(category_label,dim(df_subscribers_smoothed)[1])
# normalized data
df_subscribers_normalized_long=pivot_longer(df_subscribers_normalized,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_subscribers')
df_views_normalized_long=pivot_longer(df_views_normalized,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_views')
df_normalized_long = df_subscribers_normalized_long%>%inner_join(df_views_normalized_long,by=c('start_date','name'))
df_normalized_long$category = rep(category_label,dim(df_subscribers_normalized)[1])
```

```{r}
# save cleaned data
setwd('..')
write.csv(df_aligned_long,'./data/aligned_data.csv')
write.csv(df_smoothed_long,'./data/smoothed_data.csv')
write.csv(df_normalized_long,'./data/normalized_data.csv')
```

```{r}
# use 2019-07~09 as the baseline. Calculate the %change in weekly gained subscribers/views compared to the median values within 2019-07~09
# subscribers
subscribers_baseline = as.vector(unlist(lapply(df_subscribers_smoothed[df_subscribers_smoothed$start_date<as.Date('2019-10-01'),-1],median)))
df_subscribers_change_from_baseline = df_subscribers_smoothed[df_subscribers_smoothed$start_date>=as.Date('2019-10-01'),]
for (j in 2:dim(df_subscribers_change_from_baseline)[2]){
  df_subscribers_change_from_baseline[,j]=(df_subscribers_change_from_baseline[,j]-rep(subscribers_baseline[j-1],dim(df_subscribers_change_from_baseline)[1]))/subscribers_baseline[j-1]*100
}

#views
views_baseline = as.vector(unlist(lapply(df_views_smoothed[df_views_smoothed$start_date<as.Date('2019-10-01'),-1],median)))
df_views_change_from_baseline = df_views_smoothed[df_views_smoothed$start_date>=as.Date('2019-10-01'),]
for (j in 2:dim(df_views_change_from_baseline)[2]){
  df_views_change_from_baseline[,j]=(df_views_change_from_baseline[,j]-rep(views_baseline[j-1],dim(df_views_change_from_baseline)[1]))/views_baseline[j-1]*100
}

# change_from_baseline data
df_subscribers_change_from_baseline_long=pivot_longer(df_subscribers_change_from_baseline,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_subscribers')
df_views_change_from_baseline_long=pivot_longer(df_views_change_from_baseline,cols=codyko:thebucketlistfamily,names_to='name',values_to='weekly_views')
df_change_from_baseline_long = df_subscribers_change_from_baseline_long%>%inner_join(df_views_change_from_baseline_long,by=c('start_date','name'))
df_subscribers_change_from_baseline_long$category = rep(category_label,dim(df_subscribers_change_from_baseline)[1])
```


```{r}
# All-sample plots
setwd('..')
# create mapping for name to color, depending on category
MAPPING <- df_smoothed_long %>% distinct(category,name)
color_set =  brewer.pal(n_distinct(MAPPING$category),"Set3")
names(color_set) = unique(MAPPING$category)
plot_cols = color_set[MAPPING$category]
names(plot_cols) = MAPPING$name
# aligned data plots
p_aligned_subscribers = ggplot(df_aligned_long,mapping=aes(x=start_date, y=weekly_subscribers,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='Weekly gained subscribers')+
  scale_color_manual(values = plot_cols)+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
ggsave('./report/p_aligned_subscribers.pdf')

p_aligned_views = ggplot(df_aligned_long,mapping=aes(x=start_date, y=weekly_views,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='Weekly gained views')+
  scale_color_manual(values = plot_cols)+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
ggsave('./report/p_aligned_views.pdf')
# interpolated subscribers data
p_interpolated_subscribers = ggplot(df_subscribers_interpolated_long,mapping=aes(x=start_date, y=weekly_subscribers,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='Weekly gained subscribers')+
  scale_color_manual(values = plot_cols)+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
ggsave('./report/p_interpolated_subscribers.pdf')

# change_from_baseline plots
p_baseline_subscribers = ggplot(df_subscribers_change_from_baseline_long,mapping=aes(x=start_date, y=weekly_subscribers,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='% Change in weekly gained subscribers')+
  scale_color_manual(values = plot_cols)+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#ggplotly(p_baseline_subscribers)

p_baseline_views = ggplot(df_views_change_from_baseline_long,mapping=aes(x=start_date, y=weekly_views,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='% Change in weekly gained views')+
  scale_color_manual(values = plot_cols)+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#ggplotly(p_baseline_views)

# smoothed data plots
p_smoothed_subscribers = ggplot(df_smoothed_long,mapping=aes(x=start_date, y=weekly_subscribers,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='Weekly gained subscribers')+
  scale_color_manual(values = plot_cols)+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
ggsave('./report/p_smoothed_subscribers.pdf')

p_smoothed_views = ggplot(df_smoothed_long,mapping=aes(x=start_date, y=weekly_views,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='Weekly gained views')+
  scale_color_manual(values = plot_cols)+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
ggsave('./report/p_smoothed_views.pdf')
# normalized data
p_normalized_subscribers = ggplot(df_normalized_long,mapping=aes(x=start_date, y=weekly_subscribers,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='Weekly gained subscribers')+
  scale_color_manual(values = plot_cols) + scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
ggsave('./report/p_normalized_subscribers.pdf')

p_normalized_views = ggplot(df_normalized_long,mapping=aes(x=start_date, y=weekly_views,col=name)) + 
  geom_line(size=0.5) + theme(legend.position='none') + labs(x='Date',y='Weekly gained views')+
  scale_color_manual(values = plot_cols) + scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
ggsave('./report/p_normalized_views.pdf')
```

```{r}
## by-category plots
# subscribers
setwd('..')
p_comedy_subscribers <- df_normalized_long %>% filter(category=="comedy") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Comedy")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_comedy_subscribers <- ggplotly(p_comedy_subscribers)
p_comedy_subscribers
ggsave('./report/p_comedy_subscribers.pdf')

p_commentory_subscribers <- df_normalized_long %>% filter(category=="commentory") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Commentory")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_commentory_subscribers <- ggplotly(p_commentory_subscribers)
p_commentory_subscribers
ggsave('./report/p_commentory_subscribers.pdf')

p_educational_subscribers <- df_normalized_long %>% filter(category=="educational") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Education")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_educational_subscribers <- ggplotly(p_educational_subscribers)
p_educational_subscribers
ggsave('./report/p_educational_subscribers.pdf')

p_fashionbeauty_subscribers <- df_normalized_long %>% filter(category=="fashion&beauty") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Fashion&Beauty")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_fashionbeauty_subscribers <- ggplotly(p_fashionbeauty_subscribers)
p_fashionbeauty_subscribers
ggsave('./report/p_fashionbeauty_subscribers.pdf')

p_fitness_subscribers <- df_normalized_long %>% filter(category=="fitness") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Fitness")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_fitness_subscribers <- ggplotly(p_fitness_subscribers)
p_fitness_subscribers
ggsave('./report/p_fitness_subscribers.pdf')

p_food_subscribers <- df_normalized_long %>% filter(category=="food") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Food")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_food_subscribers <- ggplotly(p_food_subscribers)
p_food_subscribers
ggsave('./report/p_food_subscribers.pdf')

p_gaming_subscribers <- df_normalized_long %>% filter(category=="gaming") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Gaming")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_gaming_subscribers <- ggplotly(p_gaming_subscribers)
p_gaming_subscribers
ggsave('./report/p_gaming_subscribers.pdf')

p_kids_subscribers <- df_normalized_long %>% filter(category=="kids") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Kids")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_kids_subscribers <- ggplotly(p_kids_subscribers)
p_kids_subscribers
ggsave('./report/p_kids_subscribers.pdf')

p_news_subscribers <- df_normalized_long %>% filter(category=="news") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of News")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_news_subscribers <- ggplotly(p_news_subscribers)
p_news_subscribers
ggsave('./report/p_news_subscribers.pdf')

p_product_review_subscribers <- df_normalized_long %>% filter(category=="product_review") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Product review")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_product_review_subscribers <- ggplotly(p_product_review_subscribers)
p_product_review_subscribers
ggsave('./report/p_product_review_subscribers.pdf')

p_travel_subscribers <- df_normalized_long %>% filter(category=="travel") %>% ggplot(mapping = aes(x=start_date,y=weekly_subscribers,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly subscribers of Travel")+labs(x='Date',y='Weekly gained subscribers')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_travel_subscribers <- ggplotly(p_travel_subscribers)
p_travel_subscribers
ggsave('./report/p_travel_subscribers.pdf')
```

```{r}
# views
setwd('..')
p_comedy_views <- df_normalized_long %>% filter(category=="comedy") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Comedy")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_comedy_views <- ggplotly(p_comedy_views)
p_comedy_views
ggsave('./report/p_comedy_views.pdf')

p_commentory_views <- df_normalized_long %>% filter(category=="commentory") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Commentory")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_commentory_views <- ggplotly(p_commentory_views)
p_commentory_views
ggsave('./report/p_commentory_views.pdf')

p_educational_views <- df_normalized_long %>% filter(category=="educational") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Education")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_educational_views <- ggplotly(p_educational_views)
p_educational_views
ggsave('./report/p_educational_views.pdf')

p_fashionbeauty_views <- df_normalized_long %>% filter(category=="fashion&beauty") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Fashion&Beauty")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_fashionbeauty_views <- ggplotly(p_fashionbeauty_views)
p_fashionbeauty_views
ggsave('./report/p_fashionbeauty_views.pdf')

p_fitness_views <- df_normalized_long %>% filter(category=="fitness") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Fitness")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_fitness_views <- ggplotly(p_fitness_views)
p_fitness_views
ggsave('./report/p_fitness_views.pdf')

p_food_views <- df_normalized_long %>% filter(category=="food") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Food")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_food_views <- ggplotly(p_food_views)
p_food_views
ggsave('./report/p_food_views.pdf')

p_gaming_views <- df_normalized_long %>% filter(category=="gaming") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Gaming")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_gaming_views <- ggplotly(p_gaming_views)
p_gaming_views
ggsave('./report/p_gaming_views.pdf')

p_kids_views <- df_normalized_long %>% filter(category=="kids") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Kids")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_kids_views <- ggplotly(p_kids_views)
p_kids_views
ggsave('./report/p_kids_views.pdf')

p_news_views <- df_normalized_long %>% filter(category=="news") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of News")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_news_views <- ggplotly(p_news_views)
p_news_views
ggsave('./report/p_news_views.pdf')

p_product_review_views <- df_normalized_long %>% filter(category=="product_review") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Product review")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_product_review_views <- ggplotly(p_product_review_views)
p_product_review_views
ggsave('./report/p_product_review_views.pdf')

p_travel_views <- df_normalized_long %>% filter(category=="travel") %>% ggplot(mapping = aes(x=start_date,y=weekly_views,col=name))+
  geom_line(size=0.5)+
  ggtitle("Weekly views of Travel")+labs(x='Date',y='Weekly gained views')+theme(legend.position='none')+
  scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')
#p_travel_views <- ggplotly(p_travel_views)
p_travel_views
ggsave('./report/p_travel_views.pdf')
```

### Summary statistics plots
```{r, fig.height=5}
setwd('..')
p_group_median_sub <-ggplot(group_median,mapping=aes(x=start_date, y=group_median_sub)) + 
  geom_line(size=0.5) + theme_bw() + labs(x='Date',y='Median weekly gained subscribers')+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')+facet_grid(rows=vars(category))
ggsave('./report/p_group_median_sub.pdf')

p_group_median_view <-ggplot(group_median,mapping=aes(x=start_date, y=group_median_view)) + 
  geom_line(size=0.5) + theme_bw() + labs(x='Date',y='Median weekly gained views')+ scale_x_date(date_labels = "%Y %b",date_breaks = '4 month')+facet_grid(rows=vars(category))
ggplotly(p_group_median_sub)
ggsave('./report/p_group_median_view.pdf')
```
## Data analysis
```{r}
# import time_stay_at_home data
setwd('..')
time_at_home = read.csv('./data/changes-residential-duration-covid.csv')
time_at_home$Day = as.Date(time_at_home$Day)
# keep weekly data on days common to both data sets
time_at_home_weekly = time_at_home[time_at_home$Day%in%df_subscribers_normalized$start_date,]
df_subscribers_normalized_after2020 = df_subscribers_normalized[df_subscribers_normalized$start_date %in% time_at_home_weekly$Day,]
df_views_normalized_after2020= df_views_normalized[df_views_normalized$start_date %in% time_at_home_weekly$Day,]
# calculate world average of time_at_home_weekly
time_at_home_weekly_worldavg = time_at_home_weekly %>% group_by(Day) %>% summarise(world_avg=mean(residential),world_avg_low=mean(residential)-sd(residential),world_avg_high=mean(residential)+sd(residential))
# combine time_at_home data with weekly subscribers/views (wide format)
df_subscribers_normalized_with_homedata = df_subscribers_normalized_after2020 %>% inner_join(time_at_home_weekly_worldavg,by=c('start_date'='Day'))
df_views_normalized_with_homedata = df_views_normalized_after2020 %>% inner_join(time_at_home_weekly_worldavg,by=c('start_date'='Day'))

#write.csv(df_subscribers_normalized_with_homedata,'./data/subscribers_with_home.csv')
#write.csv(df_views_normalized_with_homedata,'./data/views_with_home.csv')

# plot time_at_home data (world average)
p_time_at_home = ggplot(time_at_home_weekly_worldavg,mapping=aes(x=Day,y=world_avg,ymin=world_avg_low,ymax=world_avg_high))+geom_line(size=0.5)+geom_ribbon(alpha=0.3)+labs(y='% change in time spent at home (mean+/-sd)')+theme_bw()
ggsave('../report/time_at_home.pdf')

```
### Cross correlation among channels and hierarchical clustering
```{r}
library(tidyr)
```

```{r}
subscribers_with_home <- read.csv("subscribers_with_home.csv")
views_with_home <- read.csv("views_with_home.csv")
```

```{r, echo=FALSE,include=TRUE,message=FALSE}
options(max.print=1000000)
correlationTable = function(graphs) {
  cross = matrix(nrow = length(graphs), ncol = length(graphs))
  for(graph1Id in 1:length(graphs)){
    graph1 = graphs[[graph1Id]]
    print(graph1Id)
    for(graph2Id in 1:length(graphs)) {
      graph2 = graphs[[graph2Id]]
      if(graph1Id == graph2Id){
        break;
      } else {
        correlation = ccf(graph1, graph2, lag.max = 0,plot = FALSE)
        cross[graph1Id, graph2Id] = correlation$acf[1]
      }
    }
  }
  cross
}
```

```{r}
graph_subscribers <- subscribers_with_home[,3:117]
names(graph_subscribers) <- names(subscribers_with_home)[3:117]
## Remove column 5(jennamarbles), 6(pewdiepie.x), 8(ryanhiga), 33(rclbeauty), 49(byrontalbott), 63(gamenewsofficial), 69(pewdiepie.y), 102(tldtoday)
graph_subscribers <- graph_subscribers[,-c(5,6,8,33,49,63,69,102)]
corr_subscribers <- correlationTable(graph_subscribers)
colnames(corr_subscribers) <- names(graph_subscribers)
rownames(corr_subscribers) <- names(graph_subscribers)
corr_subscribers
```

```{r}
graph_views <- views_with_home[,3:117]
names(graph_views) <- names(views_with_home)[3:117]
corr_views <- correlationTable(graph_views)
colnames(corr_views) <- names(graph_views)
rownames(corr_views) <- names(graph_views)
corr_views
```

```{r}
highCorr <- which(corr_subscribers > 0.95, arr.ind = TRUE)
highCorr <- as.data.frame(highCorr)
highCorr
```

```{r}
highCorr_view <- which(corr_views >0.90, arr.ind = TRUE)
highCorr_view <- as.data.frame(highCorr_view)
highCorr_view
```

```{r}
n <- nrow(corr_subscribers)
m <- ncol(corr_subscribers)
for(i in 1:n){
  for(j in 1:n){
    if(i==j){
      corr_subscribers[i,j] <- 1
    }else if(i < j){
      corr_subscribers[i,j] <- corr_subscribers[j,i]
    }else{
      corr_subscribers[i,j] <- corr_subscribers[i,j]
    }
  }
}
corr_subscribers
```

```{r}
n <- nrow(corr_views)
m <- ncol(corr_views)
for(i in 1:n){
  for(j in 1:m){
    if(i==j){
      corr_views[i,j] <- 1
    }else if(i < j){
      corr_views[i,j] <- corr_views[j,i]
    }else{
      corr_views[i,j] <- corr_views[i,j]
    }
  }
}
corr_views

```

```{r}
views <- as.matrix(corr_views)
write.csv(views, file = "corr_views.csv")
```


Heatmap
```{r}
library(gplots)
heatmap.2(corr_subscribers, symm = T, trace="none",key=T, revC=T)
```

```{r}
heatmap.2(corr_views, symm = T, trace = "none", key = T, revC = T)
```

# hierarchical clustering 

```{r}
normalized_data <- read.csv("normalized_data.csv")
normalized_data$start_date <- as.Date(normalized_data$start_date)
normalized_data$name <- as.factor(normalized_data$name)
normalized_data$weekly_subscribers <- as.numeric(normalized_data$weekly_subscribers)
normalized_data$weekly_views <- as.numeric(normalized_data$weekly_views)
normalized_data$category <- as.factor(normalized_data$category)
```

```{r}
library(ggplot2)
library(dplyr)
subscribers_data <- normalized_data %>% select(weekly_subscribers, start_date, name) %>% drop_na() %>% glimpse()
```
```{r}
spread_subscribers_data <- subscribers_data %>% spread(name, weekly_subscribers) %>% glimpse()
```

```{r}
subscribers_new <- t(spread_subscribers_data[-1])
subscirbers_dist <- dist(subscribers_new, method = "euclidean")
fit_1 <- hclust(subscirbers_dist, method = "ward.D")
plot(fit_1, family="Arial")
rect.hclust(fit_1, k=4, border = "cadetblue")
```

```{r}
library(ggdendro)
ggdendrogram(fit_1, rotate=TRUE, theme_dendro=FALSE)+theme_minimal()+xlab("")+ylab("")
```


```{r}
cluster_subscribers <- cutree(fit_1, k=4)
cluster_subscribers_tidy <- as.data.frame(as.table(cluster_subscribers)) %>% glimpse()
colnames(cluster_subscribers_tidy) <- c("name","cluster")
cluster_subscribers_tidy$name <- as.character(cluster_subscribers_tidy$name)

joined_subscribers_cluster <- normalized_data %>% inner_join(cluster_subscribers_tidy, by="name") %>% glimpse()
```

```{r}
table(cluster_subscribers_tidy$cluster)
```
```{r}
cluster1 <- joined_subscribers_cluster %>% filter(cluster=="1")
ggplot(cluster1,aes(start_date, weekly_subscribers))+
  geom_line(color="grey")+
  theme_minimal()+
  ggtitle("cluster 1")+
  ylab("weekly subscribers")+
  xlab("")+
  geom_smooth(method = "auto", color="red", se=F, size=0.5)+
  facet_wrap(~name)
```
```{r}
cluster2 <- joined_subscribers_cluster %>% filter(cluster=="2")
ggplot(cluster2,aes(start_date, weekly_subscribers))+
  geom_line(color="grey")+
  theme_minimal()+
  ggtitle("cluster 2")+
  ylab("weekly subscribers")+
  xlab("")+
  geom_smooth(method = "auto", color="red", se=F, size=0.5)+
  facet_wrap(~name)
```
```{r}
cluster3 <- joined_subscribers_cluster %>% filter(cluster=="3")
ggplot(cluster3,aes(start_date, weekly_subscribers))+
  geom_line(color="grey")+
  theme_minimal()+
  ggtitle("cluster 3")+
  ylab("weekly subscribers")+
  xlab("")+
  geom_smooth(method = "auto", color="red", se=F, size=0.5)+
  facet_wrap(~name)

```
```{r}
cluster4 <- joined_subscribers_cluster %>% filter(cluster=="4")
ggplot(cluster4,aes(start_date, weekly_subscribers))+
  geom_line(color="grey")+
  theme_minimal()+
  ggtitle("cluster 4")+
  ylab("weekly subscribers")+
  xlab("")+
  geom_smooth(method = "auto", color="red", se=F, size=0.5)+
  facet_wrap(~name)
```

```{r}
views_data <- normalized_data %>% select(weekly_views, start_date, name) %>% drop_na() %>% glimpse()
```
```{r}
spread_views_data <- views_data %>% spread(name, weekly_views) %>% glimpse()
```

```{r}
views_new <- t(spread_views_data[-1])
views_dist <- dist(views_new, method = "euclidean")
fit_2 <- hclust(views_dist, method = "ward.D")
plot(fit_2, family="Arial")
rect.hclust(fit_2, k=4, border = "cadetblue")
```

```{r}
library(ggdendro)
ggdendrogram(fit_2, rotate=TRUE, theme_dendro=FALSE)+theme_minimal()+xlab("")+ylab("")
```


```{r}
cluster_views <- cutree(fit_2, k=4)
cluster_views_tidy <- as.data.frame(as.table(cluster_views)) %>% glimpse()
colnames(cluster_views_tidy) <- c("name","cluster")
cluster_views_tidy$name <- as.character(cluster_views_tidy$name)

joined_views_cluster <- normalized_data %>% inner_join(cluster_views_tidy, by="name") %>% glimpse()
```

```{r}
table(cluster_views_tidy$cluster)
```
```{r}
cluster_1 <- joined_views_cluster %>% filter(cluster=="1")
ggplot(cluster_1,aes(start_date, weekly_views))+
  geom_line(color="grey")+
  theme_minimal()+
  ggtitle("cluster_1")+
  ylab("weekly views")+
  xlab("")+
  geom_smooth(method = "auto", color="red", se=F, size=0.5)+
  facet_wrap(~name)
```
```{r}
cluster_2 <- joined_views_cluster %>% filter(cluster=="2")
ggplot(cluster_2,aes(start_date, weekly_views))+
  geom_line(color="grey")+
  theme_minimal()+
  ggtitle("cluster_2")+
  ylab("weekly views")+
  xlab("")+
  geom_smooth(method = "auto", color="red", se=F, size=0.5)+
  facet_wrap(~name)
```
```{r}
cluster_3 <- joined_views_cluster %>% filter(cluster=="3")
ggplot(cluster_3,aes(start_date, weekly_views))+
  geom_line(color="grey")+
  theme_minimal()+
  ggtitle("cluster_3")+
  ylab("weekly views")+
  xlab("")+
  geom_smooth(method = "auto", color="red", se=F, size=0.5)+
  facet_wrap(~name)

```
```{r}
cluster_4 <- joined_views_cluster %>% filter(cluster=="4")
ggplot(cluster_4,aes(start_date, weekly_views))+
  geom_line(color="grey")+
  theme_minimal()+
  ggtitle("cluster_4")+
  ylab("weekly views")+
  xlab("")+
  geom_smooth(method = "auto", color="red", se=F, size=0.5)+
  facet_wrap(~name)
```

```{r}
hc_views <- hclust(dist(views_with_home[,3:120]),method = "ward.D2")
plot(hc_views)
rect.hclust(hc_views, k=4, border = 2:4)
```


### rolling-window cross correlation between time_at_home time series with category_median of subscribers and views time series
```{r}
group_median_after2020 = group_median[group_median$start_date%in%time_at_home_weekly$Day,]
group_subscribers_median_wide = spread(group_median_after2020[,1:4],category,group_median_sub)
group_subscribers_median_wide$time_at_home = time_at_home_weekly_worldavg$world_avg
group_views_median_wide = spread(group_median_after2020[,-4],category,group_median_view)
group_views_median_wide$time_at_home = time_at_home_weekly_worldavg$world_avg
```

```{r}
# cross correlation between time_at_home and category median of subscribers/views
sub_home_full_ccf = c()
view_home_full_ccf = c()
for (j in 3:(dim(group_subscribers_median_wide)[2]-1)){
  x_sub = as.numeric(unlist(group_subscribers_median_wide[,j]))
  y_sub = group_subscribers_median_wide$time_at_home
  x_views= as.numeric(unlist(group_views_median_wide[,j]))
  y_views=group_views_median_wide$time_at_home
  full_ccf_sub = ccf(x_sub,y_sub,lag.max=0,plot=FALSE)
  full_ccf_views = ccf(x_views,y_views,lag.max=0,plot=FALSE)
  sub_home_full_ccf = append(sub_home_full_ccf,round(full_ccf_sub$acf[1],digits = 2))
  view_home_full_ccf =append(view_home_full_ccf,round(full_ccf_views$acf[1],digits=2))
}
df_ccf_hometime = as.data.frame(cbind(colnames(group_subscribers_median_wide)[3:(dim(group_subscribers_median_wide)[2]-1)],sub_home_full_ccf,view_home_full_ccf))
colnames(df_ccf_hometime)=c('category','subscribers','views')
write.csv(df_ccf_hometime,'../report/ccf_with_hometime_full.csv')

df_ccf_hometime_long = pivot_longer(df_ccf_hometime,col=subscribers:views,names_to = 'Measures',values_to = 'Cross_correlation')
df_ccf_hometime_long$Cross_correlation=as.numeric(df_ccf_hometime_long$Cross_correlation)

library(ggthemes)
pdf('../report/p_ccf_with_hometime_full.pdf')
ggplot(df_ccf_hometime_long,aes(x=category,y=Cross_correlation,fill=Measures))+geom_bar(stat = 'identity',position='dodge')+theme_tufte()+geom_rangeframe()+theme(legend.position = 'bottom',axis.text.x = element_text(angle = 45, vjust=0.5))
dev.off()
```

```{r}
library(tseries)
# define function to calculate rolling window cross correlation between two univariate time series data
rolling_window_ccf <-function(x,y,window_size,step_size,lag.max){
  t_start = 1
  t_end = t_start+window_size
  lags = -lag.max:lag.max
  df_ccf = as.data.frame(lags)
  dates_index = c()
  while(t_end<=length(x)){
    x_t = x[t_start:t_end]
    y_t = y[t_start:t_end]
    ccf_t = ccf(x_t,y_t,lag.max,plot=FALSE)
    if (sum(ccf_t$lag!=df_ccf$lags)>0){print('Error with lag.max setting')}
    else {
      df_ccf = cbind(df_ccf,ccf_t$acf)
      dates_index=c(dates_index,t_start)
    }
    t_start = t_start+step_size
    t_end = t_end+step_size
  }
  colnames(df_ccf)[-1]=dates_index
  return(df_ccf)
}

# calculate rolling window time-lagged ccf matrix between time_at_home time series and each of the channel category median 
# subscribers
subscribers_home_ccf_rw = list()
for (j in 3:(dim(group_subscribers_median_wide)[2]-1)){
  category = colnames(group_subscribers_median_wide)[j]
  x = as.numeric(unlist(group_subscribers_median_wide[,j]))
  y = group_subscribers_median_wide$time_at_home
  # calculate rolling window time-lagged cross-correlation between category median subscribers (x) and time_at_home (y)
  # since the ccf function calculate correlation between x[x+k] and y[x] for a given lag value k.
  # If the peak ccf occurs at a positive lag value, then time_at_home time series leads the channel weekly gained subscribers time series, vice versa.
  ccf_category = rolling_window_ccf(x,y,window_size = 24,step_size = 2,lag.max = 12)
  subscribers_home_ccf_rw[[category]]=ccf_category
}

# views
views_home_ccf_rw = list()
for (j in 3:(dim(group_views_median_wide)[2]-1)){
  category = colnames(group_views_median_wide)[j]
  x = as.numeric(unlist(group_views_median_wide[,j]))
  y = group_views_median_wide$time_at_home

  ccf_category = rolling_window_ccf(x,y,window_size = 24,step_size = 2,lag.max = 12)
  views_home_ccf_rw[[category]]=ccf_category
}
```

```{r}
# plot heatmaps of rolling window ccf
# subscribers
library(ComplexHeatmap)
for (i in 1:length(subscribers_home_ccf_rw)){
  df = subscribers_home_ccf_rw[[i]]
  ccf_forplot = t(as.matrix(df[,-1]))
  row_lab = group_subscribers_median_wide$start_date[as.numeric(rownames(ccf_forplot))]
  col_lab = df$lags
  filename = paste(names(subscribers_home_ccf_rw)[i],'png',sep='.')
  png(paste('../report/heatmap_sub',filename,sep='/'),width = 600,height=600)
  heatmap=Heatmap(ccf_forplot,cluster_rows = FALSE,cluster_columns = FALSE,
          row_labels = row_lab,
          column_labels = col_lab,
          row_title = 'Window start date',
          column_title = 'Week lags',
          column_title_side='bottom',
          name=names(subscribers_home_ccf_rw)[i],
          width=unit(10,'cm'))
  print(heatmap)
  dev.off()
} 

# views
for (i in 1:length(views_home_ccf_rw)){
  df = views_home_ccf_rw[[i]]
  ccf_forplot = t(as.matrix(df[,-1]))
  row_lab = group_views_median_wide$start_date[as.numeric(rownames(ccf_forplot))]
  col_lab = df$lags
  filename = paste(names(views_home_ccf_rw)[i],'png',sep='.')
  png(paste('../report/heatmap_view',filename,sep='/'),width = 600,height=600)
  heatmap=Heatmap(ccf_forplot,cluster_rows = FALSE,cluster_columns = FALSE,
          row_labels = row_lab,
          column_labels = col_lab,
          row_title = 'Window start date',
          column_title = 'Week lags',
          column_title_side='bottom',
          name=names(views_home_ccf_rw)[i],
          width=unit(10,'cm'))
  print(heatmap)
  dev.off()
} 



```

